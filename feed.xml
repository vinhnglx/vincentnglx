<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>VincentNguyen</title>
  <subtitle>Code to change!</subtitle>
  <id>http://vincent.com</id>
  <link href="http://vincent.com"/>
  <link href="http://vincent.com/feed.xml" rel="self"/>
  <updated>2017-03-03T23:40:00+08:00</updated>
  <author>
    <name>Vincent</name>
  </author>
  <entry>
    <title>How to scrape a web page to find duplicate contents</title>
    <link rel="alternate" href="http://vincent.com/2017/03/03/scrape-web-page-to-find-duplicate-contents/"/>
    <id>http://vincent.com/2017/03/03/scrape-web-page-to-find-duplicate-contents/</id>
    <published>2017-03-03T23:40:00+08:00</published>
    <updated>2017-03-04T17:47:23+08:00</updated>
    <author>
      <name>Vincent</name>
    </author>
    <summary type="html">&lt;p&gt;I recently tried to submit my job application to a few companies, unfortunately, I still can't get hired but I passed some rounds
for technical test. This post is a technical test from a company in Singapore, they want to me write a tool to
find duplicated...&lt;/p&gt;</summary>
    <content type="html">&lt;p&gt;I recently tried to submit my job application to a few companies, unfortunately, I still can't get hired but I passed some rounds
for technical test. This post is a technical test from a company in Singapore, they want to me write a tool to
find duplicated contents from their website.&lt;/p&gt;

&lt;p&gt;From the requirement, you will know we need to write a scraping tool and an algorithm to find duplicated contents.
So this post will have two sections: &lt;em&gt;how to scrape a site&lt;/em&gt; and &lt;em&gt;how to find duplicated contents&lt;/em&gt;?&lt;/p&gt;

&lt;h2 id="how-to-scrape-a-site"&gt;How to scrape a site&lt;/h2&gt;

&lt;p&gt;We will open any website, for example, &lt;a href="https://www.viki.com"&gt;Viki&lt;/a&gt;. At the first glance, you will find the problem.&lt;/p&gt;

&lt;p&gt;&lt;img src="https://docs.google.com/uc?id=0B-S3PHiYZOY1S0otclJ0OHFOckk" alt="Viki - Homepage" /&gt;&lt;/p&gt;

&lt;p&gt;We can say: Contents are considered duplicate if they had the same URL, title, and image_src.&lt;/p&gt;

&lt;p&gt;What's next step? Will we write a code to parse HTML contents?&lt;/p&gt;

&lt;p&gt;No, we need to check again because you know, a famous site like Viki, I don't believe they only use one way to render contents.&lt;/p&gt;

&lt;p&gt;So, what should I do? Okay - We will the javascript on this website to find which contents will be rendered by Javascript. Wait!!! Perharps some people will ask me why I do that? Why I disable javascript?&lt;/p&gt;

&lt;p&gt;My answer is very simple, trust me, because this will directly affect the way you will scrape the site!&lt;/p&gt;

&lt;p&gt;So, this is a screenshot after I turned off Javascript&lt;/p&gt;

&lt;p&gt;&lt;img src="https://docs.google.com/uc?id=0B-S3PHiYZOY1Y3pjanhsSFJXbk0" alt="Viki - After disabled JS - Homepage" /&gt;&lt;/p&gt;

&lt;p&gt;There is a difference between these images - You can't find the &lt;strong&gt;"TOP PICKS FOR YOU"&lt;/strong&gt; section in the screenshot that I disabled Javascript.
This mean maybe this section is rendered from AJAX call by using Javascript.&lt;/p&gt;

&lt;p&gt;Next step, we will use Chrome Developer Tool to find the common HTML structure of these duplicated contents.&lt;/p&gt;

&lt;p&gt;&lt;img src="https://docs.google.com/uc?id=0B-S3PHiYZOY1Szl5a0V5UG96aHM" alt="Viki - Inspect common html structure" /&gt;&lt;/p&gt;

&lt;p&gt;These elements has common class &lt;code&gt;"thumbnail js-express-player"&lt;/code&gt; and structure inside &lt;code&gt;&amp;lt;div class="thumbnail-wrapper"&amp;gt;&amp;lt;/div&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;div class="thumbnail-description"&amp;gt;&amp;lt;/div&amp;gt;&lt;/code&gt;.
We will parse to get html contents based on these classes.&lt;/p&gt;

&lt;p&gt;Here is the final decision: AFTER the page finished loading contents, we will crawl the site, parse the HTML contents.&lt;/p&gt;

&lt;p&gt;My favorite language is Ruby - and in Ruby, I can use &lt;a href="https://ruby-doc.org/stdlib-2.1.0/libdoc/open-uri/rdoc/OpenURI.html"&gt;open-uri&lt;/a&gt;
or few gems such as &lt;a href="http://www.nokogiri.org/"&gt;nokogiri&lt;/a&gt;, &lt;a href="https://github.com/sparklemotion/mechanize"&gt;mechanize&lt;/a&gt; to crawl and parse HTML contents.&lt;/p&gt;

&lt;p&gt;But we want to parse the content that AJAX loads its, hence these gems can't handle it.
In this case, we can use &lt;a href="https://github.com/SeleniumHQ/selenium/wiki/Ruby-Bindings"&gt;Selenium-Webdriver&lt;/a&gt; with a headless webkit - &lt;a href="https://github.com/ariya/phantomjs"&gt;PhantomJS&lt;/a&gt; to crawl the page and &lt;a href="https://github.com/ariya/phantomjs"&gt;Nokogiri&lt;/a&gt; to parse.&lt;/p&gt;

&lt;p&gt;Basically the code will be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  @driver = Selenium::WebDriver.for(:phantomjs)
@driver.navigate.to('https://www.viki.com')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, we will use Nokogiri to parse HTML contents&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  page_source = Nokogiri::HTML(@driver.page_source)
body = page_source.css('.thumbnail.js-express-player')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As I said before, we will parse title, url and image_src and these will be located on an object: &lt;code&gt;{title: "...", image_src: "...", url: "xxx"}&lt;/code&gt;.
Hence the code will be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  body.map {|element|
  {
    "title" =&amp;gt; element.css('.thumb-title').text.strip,
    "image_src" =&amp;gt; element.css('.thumbnail-img img').attribute('src').text,
    "url" =&amp;gt; element.css('.thumbnail-img').attribute('href').text
  }.to_s
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we get a list of objects then we are ready to go to step two: Find the duplicated contents from this list.&lt;/p&gt;

&lt;h2 id="how-to-find-duplicated-contents"&gt;How to find duplicated contents&lt;/h2&gt;

&lt;p&gt;We had the array of objects so how to find identical objects in array? We implement a loop and compare one by one to find the identical object?
Huh, that's solution will work but the Big O notation is &lt;strong&gt;O(n^2)&lt;/strong&gt; - the code will run very slow.&lt;/p&gt;

&lt;p&gt;So what's the better solution? Did you remember the &lt;a href="https://en.wikipedia.org/wiki/MD5"&gt;MD5 algorithm&lt;/a&gt;? The MD5 algorithm is a widely used hash function producing a 128-bit hash value.
It can be used as a checksum to verify data integrity.&lt;/p&gt;

&lt;p&gt;In this case, I need a cryptographic hash that takes content and returns a fixed bit string. Objects have the same content will be grouped with identical hash.&lt;/p&gt;

&lt;p&gt;Ruby have a library called &lt;a href="https://ruby-doc.org/stdlib-2.1.0/libdoc/digest/rdoc/Digest.html"&gt;Digest/MD5&lt;/a&gt; can help me encrypt.&lt;/p&gt;

&lt;p&gt;Below is my code sample, the final result is hash contains identical objects, and of course the Big O notation is &lt;strong&gt;O(n)&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  hash = {}
raw.each do |el|
  key = Digest::MD5.hexdigest(el).to_sym
  hash.has_key?(key) ? hash[key].push(el) : hash[key] = [el]
end

hash.delete_if { |key, value| value.size == 1 }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;From this technical test, I develop a tool can extend to find duplicated contents from many websites. Source code available at &lt;a href="https://github.com/vinhnglx/dupco"&gt;vinhnglx/dupco&lt;/a&gt;&lt;/p&gt;
</content>
  </entry>
</feed>
